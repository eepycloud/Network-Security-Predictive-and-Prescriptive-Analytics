# -*- coding: utf-8 -*-
"""Mohammad_Thaher_21110274.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yptmv1PzLoEhPZM6rXaVMn4eY1lXRjUT
"""

import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical operations
import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations
import seaborn as sns  # For data visualization based on matplotlib
from sklearn.feature_selection import SelectKBest, f_regression, RFE, f_classif  # For feature selection
from sklearn.model_selection import train_test_split  # For splitting the data into training and test sets
from sklearn.linear_model import LinearRegression  # For the Linear Regression model
from sklearn.tree import DecisionTreeRegressor  # Decision Tree Regressor model
from sklearn.ensemble import RandomForestRegressor  # Random Forest Regressor model
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # For model evaluation
from sklearn.preprocessing import LabelEncoder  # For categorical data encoding, import LabelEncoder from sklear

# Load the data
network_data = pd.read_csv('WSNBFSFdataset V2.csv')

# Displaying the dataframe
network_data.head()

# Select features for prediction
X = network_data.drop(columns=['rate_of_energy_consumption'])
y = network_data['rate_of_energy_consumption']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Encode the target variable (if it's categorical)
label_encoder = LabelEncoder()
label_encoder.fit(y_train.append(y_test))
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Feature selection using SelectKBest
selector_kbest = SelectKBest(score_func=f_regression, k=5)
selector_kbest.fit(X_train.select_dtypes(include=[np.number]), y_train)
scores_kbest = pd.DataFrame({'Feature': X_train.select_dtypes(include=[np.number]).columns, 'Score': selector_kbest.scores_})
print("SelectKBest Results:")
print(scores_kbest.sort_values(by='Score', ascending=False))

# Filter the dataframe to include only the selected features
selected_features_kbest = scores_kbest[selector_kbest.get_support()]

print("SelectKBest Results (Selected Features):")
print(selected_features_kbest)

# Feature selection using RFE
estimator = LinearRegression()
selector_rfe = RFE(estimator, n_features_to_select=5, step=1)
selector_rfe.fit(X_train.select_dtypes(include=[np.number]), y_train)
scores_rfe = pd.DataFrame({'Feature': X_train.select_dtypes(include=[np.number]).columns, 'Selected': selector_rfe.support_, 'Ranking': selector_rfe.ranking_})
sorted_scores_rfe = scores_rfe.sort_values(by='Ranking')
print("\nRFE Results (Sorted by Ranking):")
print(sorted_scores_rfe)

# Filter the dataframe to include only the selected features
selected_features_rfe = scores_rfe[scores_rfe['Selected']]

print("\nRFE Results (Selected Features):")
print(selected_features_rfe)

# Define the feature selection techniques
feature_selection_techniques = {
    "SelectKBest": SelectKBest(score_func=f_regression, k=10),
    "RFE": RFE(estimator=LinearRegression(), n_features_to_select=10, step=1)
}

# Define regression models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor()
}

# Initialize dictionaries to store evaluation results
evaluation_results = {model_name: {"MAE": [], "MSE": [], "RMSE": [], "R2": []} for model_name in models}

# Initialize an empty DataFrame to store results
results_df = pd.DataFrame(columns=["FS Technique", "Model", "MAE", "MSE", "RMSE", "R2"])

# Perform feature selection, model training, and evaluation for 30 iterations
for iteration in range(30):
    for fs_name, fs_technique in feature_selection_techniques.items():
        X_selected = fs_technique.fit_transform(X_train.select_dtypes(include=[np.number]), y_train)

        for model_name, model in models.items():
            model.fit(X_selected, y_train)
            X_test_selected = fs_technique.transform(X_test.select_dtypes(include=[np.number])) # transform X_test using the same feature selection technique

            y_pred = model.predict(X_test_selected)

            # Calculate evaluation metrics
            mae = mean_absolute_error(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test, y_pred)

            # Store evaluation results in the dictionary
            evaluation_results[model_name]["MAE"].append(mae)
            evaluation_results[model_name]["MSE"].append(mse)
            evaluation_results[model_name]["RMSE"].append(rmse)
            evaluation_results[model_name]["R2"].append(r2)

            # Append results to the DataFrame
            result_row = {
                "FS Technique": fs_name,
                "Model": model_name,
                "MAE": mae,
                "MSE": mse,
                "RMSE": rmse,
                "R2": r2
            }
            results_df = results_df.append(result_row, ignore_index=True)

# Group by "FS Technique" and "Model" columns and calculate the mean for each group
average_results_df = results_df.groupby(['FS Technique', 'Model']).mean().reset_index()

print("Average Results:")
print(average_results_df)

# Create box plots for RMSE and R2 for SelectKBest and RFE
for metric in ["RMSE", "R2"]:
    for technique in ["SelectKBest", "RFE"]:
        plt.figure(figsize=(12, 6))
        data = results_df[(results_df["FS Technique"] == technique)]

        sns.boxplot(x="Model", y=metric, data=data, palette="Set3", showmeans=True, meanline=True)
        plt.title(f"{metric} Comparison ({technique})")
        plt.xlabel("Model")
        plt.ylabel(metric)
        plt.xticks(rotation=45)
        plt.show()

# Create bar plots for MSE, RMSE and R2 for SelectKBest and RFE
for metric in ["RMSE", "R2","MSE"]:
    for technique in ["SelectKBest", "RFE"]:
        plt.figure(figsize=(12, 6))

        # Create a bar for each model
        for model_name in models.keys():
            metric_values = average_results_df[
                (average_results_df["FS Technique"] == technique) &
                (average_results_df["Model"] == model_name)][metric]

            plt.bar(model_name, metric_values, label=model_name, alpha=0.7)

        plt.title(f"{metric} Comparison ({technique})")
        plt.xlabel("Model")
        plt.ylabel(metric)
        plt.legend()
        plt.show()
